{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats as sm_stats\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import math"
   ]
  },
  {
   "source": [
    "Nacitanie datasetov na predspracovanie"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_data = pd.read_csv(\"Dataset/personal_train.csv\")\n",
    "other_data = pd.read_csv(\"Dataset/other_train.csv\")"
   ]
  },
  {
   "source": [
    "Funkcia ktora zavola vsetky potrebne upravovacie funkcie a vytvori finalny dataset a exportuje ho do .csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(personal_data, other_data):\n",
    "    # remove unwanted column\n",
    "    if 'Unnamed: 0' in personal_data:\n",
    "        del personal_data['Unnamed: 0']\n",
    "    if 'Unnamed: 0' in other_data:\n",
    "        del other_data['Unnamed: 0']\n",
    "    # merge datasets to create single large dataset with usefull data so it's easier to create graphs and analysis\n",
    "    merged_medical_info_dataset = get_aggregated_data(other_data)\n",
    "    usefull_dataset = personal_data.merge(merged_medical_info_dataset, on=['name', 'address'], how='outer')\n",
    "    # replace NaN values with medians for numeric or most frequent values for categorical values\n",
    "    usefull_dataset = remove_nans(usefull_dataset)\n",
    "    # reduce number of features to improve performance\n",
    "    usefull_dataset = feature_reduction(usefull_dataset)\n",
    "    #backup values of class before scaling data\n",
    "    backup_class = usefull_dataset[['age', 'class']]\n",
    "    # detect and correct outliers\n",
    "    usefull_dataset = outlier_detection(usefull_dataset)\n",
    "    # scale and transform data\n",
    "    usefull_dataset = transform(usefull_dataset)\n",
    "    # remap class values\n",
    "    usefull_dataset['class'] = backup_class['class'].values\n",
    "    #export\n",
    "    usefull_dataset.to_csv('Dataset/preprocessed_dataset.csv', index=False)\n",
    "    return usefull_dataset\n"
   ]
  },
  {
   "source": [
    "Funkcia na vytiahnutie a nacitanie agragovvanych udajov v datasete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_data(dataset):\n",
    "    unique_medical_name_dataset = dataset.dropna(subset=['medical_info']).drop_duplicates('name')\n",
    "    # create a dataset from 'medical_info' attribute transform it to JSON format and read as object to create dataframe\n",
    "    medical_data_objects = []\n",
    "    for index, record in unique_medical_name_dataset.iterrows():\n",
    "        if isinstance(record['medical_info'], float):\n",
    "            continue\n",
    "        medical_object = json.loads(record['medical_info'].replace(\"\\'\", '\\\"').replace(':\\\"',':').replace('\\\",',',').replace('\\\"}','}'))\n",
    "        medical_object['name'] = record['name']\n",
    "        medical_data_objects.append(medical_object)\n",
    "    medical_info_dataset = pd.DataFrame(medical_data_objects)\n",
    "    merged_medical_info_dataset = unique_medical_name_dataset.merge(medical_info_dataset, on=['name'], how='outer').drop('medical_info', axis='columns')\n",
    "    return merged_medical_info_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zbavovanie sa nan pomocou pipeline\n",
    "def remove_nans(dataset):\n",
    "    test = dataset\n",
    "    # V stlpci class nan nenahrazdujeme medianom alebo najcastejsou hodnotou lebo by to mohlo dalej ovplivnovat vysledky\n",
    "    # Takze radsej tie riadky ktore maju nan v stlpci class odstranujeme\n",
    "    test = test[test['class'].isnull()==False]\n",
    "    test.replace('?',np.NaN,inplace=True)\n",
    "\n",
    "    # Vytiahneme si stlpce ktore obsahuju numericky a objektove hodnoty zvlast aby sme na kadzy druh vedeli aplikovat iny pipeline\n",
    "    numeric_features = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "    num = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='median'))])\n",
    "    cat = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='most_frequent'))])\n",
    "    # na kazdy stlpec v tabulke aplikujeme pipeline tranformaciu pomocou column transformeru\n",
    "    full = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num',  num, numeric_features),\n",
    "            ('cat', cat, categorical_features)])\n",
    "    # pre opetovne zlepenie tabulky si spojime nazvy stlpcov lebo pipeline po fiit_tranformacii vyhodi tabulku s najprv numerickymi datami \n",
    "    # a potom s objektovymi\n",
    "    columns = numeric_features.tolist() + categorical_features.tolist()\n",
    "    dtype = {}\n",
    "    for column in columns:\n",
    "        dtype[column]=(test.dtypes.to_dict()[column])\n",
    "\n",
    "    return pd.DataFrame(full.fit_transform(test), columns=columns,index=test.index).astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature reduction\n",
    "\n",
    "Funkcia na zredukovanie nepotrebnych atributov a vyselektovanie k najlepsich atributov ktore najviac kontribuuju k spravnej predpovedi vysledku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction(dataset):\n",
    "    test = dataset\n",
    "    test = test[test['class'].isnull()==False]\n",
    "    test.replace('?',np.NaN,inplace=True)\n",
    "    \n",
    "    numeric_features = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = test.select_dtypes(include=['object']).columns\n",
    "    columns_to_remove = ['name', 'education-num', 'capital-gain', 'capital-loss', 'address', 'date_of_birth', 'occupation', 'marital-status','relationship', 'education', 'native-country', 'workclass', 'income', 'race']\n",
    "\n",
    "    dataset['pregnant'] = dataset['pregnant'].apply(lambda value: 0 if re.search('T', value, re.IGNORECASE) else 1)\n",
    "    # manualne odstranenie stlpcov ktore podla nas nemaju vplyv na vyslednu hodnotu (vacsinou kategoricke atributy)\n",
    "    dataset_for_UST = dataset.drop(columns_to_remove, axis=1, errors='ignore')\n",
    "    columns_to_remove.append('pregnant')\n",
    "    categorical_features = categorical_features.drop(columns_to_remove, errors='ignore')\n",
    "\n",
    "    dataset_for_UST = pd.get_dummies(dataset_for_UST, columns=categorical_features)\n",
    "\n",
    "    # Feature Selection with Univariate Statistical Tests\n",
    "    # pouzijeme statisticke tety aby sme vybrali K najlepsich atributov ktore najviac vplyvaju na vysledok\n",
    "\n",
    "    UST_X = dataset_for_UST.drop('class', axis=1)\n",
    "    UST_y = dataset_for_UST['class']\n",
    "    # feature extraction\n",
    "    test = SelectKBest(score_func=f_classif, k=13)\n",
    "    fit = test.fit(UST_X, UST_y)\n",
    "    # summarize scores\n",
    "    #vidime ze niektore atributy maju miniaturny vplyv na vysledok priam az zanedbatelny tak ich vyhodime aby sme zrychlili vypocet modelu\n",
    "    print(fit.scores_)\n",
    "    features_columns = {}\n",
    "    for i in range(len(fit.scores_)):\n",
    "        features_columns[UST_X.columns[i]] = fit.scores_[i]\n",
    "    columns_by_rank = {k: features_columns[k] for k in sorted(features_columns, key=features_columns.get, reverse=True)}\n",
    "    columns_to_remove = list(columns_by_rank.keys())[13-(len(features_columns)):]\n",
    "    return dataset_for_UST.drop(columns_to_remove, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odstranenie outlierov\n",
    "zvolili sme metodu IRQ na detekciu outlierov a hodnoty ktore su outlieri nahradime krajnymi hodnotami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(dataset):\n",
    "    for column in dataset.columns:\n",
    "        Q1 = dataset[column].quantile(0.25)\n",
    "        Q3 = dataset[column].quantile(0.75)\n",
    "\n",
    "        IRQ = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - (1.5*IRQ)\n",
    "        upper_bound = Q3 + (1.5*IRQ)\n",
    "        if(dataset[column].dtype == 'int64'):\n",
    "            dataset[column] = dataset[column].apply(lambda val: math.floor(lower_bound) if val < lower_bound else math.floor(upper_bound) if val > upper_bound else val)\n",
    "        else:\n",
    "            dataset[column] = dataset[column].apply(lambda val: lower_bound if val < lower_bound else upper_bound if val > upper_bound else val)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transofrmation\n",
    "\n",
    "pouzijeme Robust scaler na transformaciu dat do podobnych rozsahov a power transformer na transformaciu rozdelenia dat blizsie ku normalovej distribucii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset):\n",
    "    scaler = RobustScaler()\n",
    "    scaled = scaler.fit_transform(dataset)\n",
    "    dataset = pd.DataFrame(scaled, columns=dataset.columns)\n",
    "\n",
    "    power = PowerTransformer(method='yeo-johnson', standardize=True) \n",
    "    data_trans = power.fit_transform(dataset)\n",
    "    return pd.DataFrame(data_trans, columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_data(personal_data, other_data)\n",
    "dataset.info()"
   ]
  },
  {
   "source": [
    "Pozrieme sa na symetrickost a rozdelenie jednotlivych atributov v spracovanom datasete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataset.columns:\n",
    "    skewness = stats.skew(dataset[column])\n",
    "    tab = 20 - len(column)\n",
    "    if skewness <= 0.5 and skewness >= -0.5:\n",
    "        print(column,' ' * tab, 'symmetrical    ', skewness)\n",
    "    elif skewness < -0.5:\n",
    "        print(column,' ' * tab, 'negative skew  ', skewness)\n",
    "    else:\n",
    "        print(column,' ' * tab, 'positive skew  ', skewness)\n",
    "\n",
    "for column in dataset.columns:\n",
    "    kurtosis = stats.kurtosis(dataset[column])\n",
    "    tab = 20 - len(column)\n",
    "    if kurtosis <= 3.5 and kurtosis >= 2.5:\n",
    "        print(column,' ' * tab, 'symmetrical    ', skewness)\n",
    "    elif skewness < 2.5:\n",
    "        print(column,' ' * tab, 'in middle      ', skewness)\n",
    "    else:\n",
    "        print(column,' ' * tab, 'on outer       ', skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prieskumna analyza\n",
    "2 Datasety sme spojili do jedneho a odstranili sme duplicitne hodnoty a nulove hodnoty pre atribut class ktory je pre nas klucovy a nemozeme ho doplnit\n",
    "\n",
    "Dalej sme zredukovali pocet atributov na take ktore najviac kontribuuju k vysledku a cyhodili take ktore pravdepodobne vobec nekontribuuju.\n",
    "¨\n",
    "Data sme taktiez transformovali aby sa viac podobali normalnej distribucii a pochadzali z podobnych rozsahov hodnot.\n",
    "\n",
    "Tieto transformacie zmenili vyzor dat a v nasledujucej sekcii sa pozrieme na to ako sa zmenili distribucie a dalsie statisticke atributy dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['skewness_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['mean_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_glucose'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['kurtosis_glucose'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['skewness_glucose'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozeme si vsimnut ze vsetky atributy maju velmi pdobne rozsahy ale velmi rozdielne priemery avsak podobne standardne distribucie. toto je vysledok transformacii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='class', kind='reg')"
   ]
  },
  {
   "source": [
    "Kedze je cely dataset uz ako numericke hodnoty mozeme ho naraz vyhodnotit pre korelacia meddzi atributmi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parova analyza a zistovanie zavislosti veku(age) a sklonu glukozy(skewness_glucose)\n",
    "\n",
    "pozrieme sa na to ako vyzeraju distribucie tychto atributov a nasledne ich zanalyzujeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(dataset['age'], line='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(dataset['skewness_glucose'], line='s')"
   ]
  },
  {
   "source": [
    "skewness_glucose z predoslej analyzy\n",
    "\n",
    "![skewness_glucose](./skewness_glucose.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Atribut vek vyzera ako by mohol pochadzat z normalnej distribucie aj ked chvosty sa uz odklanaju\n",
    "\n",
    "Atribut skewness glucose ma odlahle chvosty ale vdaka transformacii je to menej odchylene ako pred transformaciou"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=dataset['age'], y=dataset['skewness_glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = stats.pearsonr(dataset['age'], dataset['skewness_glucose']) \n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre atributy age a skewness glucose sa nam corelacia takmer nezmenila predtym: 0.250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "stat, p =kruskal(dataset['age'], dataset['skewness_glucose'])\n",
    "\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['age'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['skewness_glucose'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parova analyza mean_glucose a mean_oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = stats.pearsonr(dataset['mean_oxygen'], dataset['mean_glucose']) \n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "source": [
    "Pre tieto atributy sa nam zmenila korelacia viac a to z -0.322 na 0.030 moze to byt dosledkom odstranenia outlierov ktori tlacili korelaciu do negativnych hodnot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "stat, p =kruskal(dataset['mean_oxygen'], dataset['mean_glucose'])\n",
    "\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['mean_oxygen'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['mean_glucose'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testovanie predikcie class pomocou pipeline a troch classifierov z sklearn.ensemble\n",
    "# najprv si rozdelime dataset na dve casti aby sme si pomocou train_test_split vedeli spravit trenovacie a testovacie data na predikciu\n",
    "X = dataset.drop('class', axis=1)\n",
    "y = dataset['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "classifiers = [\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "# nakolko data su uz spracovane nebolo treba v pipeline ich transformovat a vyhadzovat nan hodnoty\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    print(classifier)\n",
    "    # print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    falseP = 0\n",
    "    falseN = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_test.tolist()[i]:\n",
    "            if y_pred[i] == 1:\n",
    "                falseP += 1\n",
    "            else: \n",
    "                falseN += 1\n",
    "                \n",
    "    print(\"False Positive: {}\".format(falseP))\n",
    "    print(\"False Negative: {}\".format(falseN))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}