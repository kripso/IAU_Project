{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats as sm_stats\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_data = pd.read_csv(\"Dataset/personal_valid.csv\")\n",
    "other_data = pd.read_csv(\"Dataset/other_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(personal_data, other_data):\n",
    "    if 'Unnamed: 0' in personal_data:\n",
    "        del personal_data['Unnamed: 0']\n",
    "    if 'Unnamed: 0' in other_data:\n",
    "        del other_data['Unnamed: 0']\n",
    "    # merge datasets to create single large dataset with usefull data so it's easier to create graphs and analysis\n",
    "    merged_medical_info_dataset = get_aggregated_data(other_data)\n",
    "    usefull_dataset = personal_data.merge(merged_medical_info_dataset, on=['name', 'address'], how='outer')\n",
    "    usefull_dataset = remove_nans(usefull_dataset)\n",
    "    usefull_dataset = feature_reduction(usefull_dataset)\n",
    "    backup_age_class = usefull_dataset[['age', 'class']]\n",
    "    usefull_dataset = outlier_detection(usefull_dataset)\n",
    "    usefull_dataset = transform(usefull_dataset)\n",
    "    usefull_dataset['age'] = backup_age_class['age'].values\n",
    "    usefull_dataset['class'] = backup_age_class['class'].values\n",
    "\n",
    "    usefull_dataset.to_csv('Dataset/preprocessed_dataset.csv', index=False)\n",
    "    return usefull_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_data(dataset):\n",
    "    unique_medical_name_dataset = dataset.dropna(subset=['medical_info']).drop_duplicates('name')\n",
    "    # create a dataset from 'medical_info' attribute\n",
    "    medical_data_objects = []\n",
    "    for index, record in unique_medical_name_dataset.iterrows():\n",
    "        if isinstance(record['medical_info'], float):\n",
    "            continue\n",
    "        medical_object = json.loads(record['medical_info'].replace(\"\\'\", '\\\"').replace(':\\\"',':').replace('\\\",',',').replace('\\\"}','}'))\n",
    "        medical_object['name'] = record['name']\n",
    "        medical_data_objects.append(medical_object)\n",
    "    medical_info_dataset = pd.DataFrame(medical_data_objects)\n",
    "    merged_medical_info_dataset = unique_medical_name_dataset.merge(medical_info_dataset, on=['name'], how='outer').drop('medical_info', axis='columns')\n",
    "    return merged_medical_info_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(dataset):\n",
    "    test = dataset\n",
    "    test = test[test['class'].isnull()==False]\n",
    "    test.replace('?',np.NaN,inplace=True)\n",
    "    X = test.drop('class', axis=1)\n",
    "    y = test['class']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    numeric_features = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "    num = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='median'))])\n",
    "    cat = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='most_frequent'))])\n",
    "    full = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num',  num, numeric_features),\n",
    "            ('cat', cat, categorical_features)])\n",
    "\n",
    "    columns = numeric_features.tolist() + categorical_features.tolist()\n",
    "    dtype = {}\n",
    "    for column in columns:\n",
    "        dtype[column]=(test.dtypes.to_dict()[column])\n",
    "\n",
    "    return pd.DataFrame(full.fit_transform(test), columns=columns,index=test.index).astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction(dataset):\n",
    "    test = dataset\n",
    "    test = test[test['class'].isnull()==False]\n",
    "    test.replace('?',np.NaN,inplace=True)\n",
    "    X = test.drop('class', axis=1)\n",
    "    y = test['class']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    numeric_features = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = test.select_dtypes(include=['object']).columns\n",
    "    columns_to_remove = ['name', 'education-num', 'capital-gain', 'capital-loss', 'address', 'date_of_birth', 'occupation', 'marital-status','relationship', 'education', 'native-country', 'workclass', 'income', 'race']\n",
    "\n",
    "    dataset['pregnant'] = dataset['pregnant'].apply(lambda value: 0 if re.search('T', value, re.IGNORECASE) else 1)\n",
    "    # manualne odstranenie stlpcov ktore podla nas nemaju vplyv na vyslednu hodnotu (vacsinou kategoricke atributy)\n",
    "    dataset_for_UST = dataset.drop(columns_to_remove, axis=1, errors='ignore')\n",
    "    columns_to_remove.append('pregnant')\n",
    "    categorical_features = categorical_features.drop(columns_to_remove, errors='ignore')\n",
    "\n",
    "    dataset_for_UST = pd.get_dummies(dataset_for_UST, columns=categorical_features)\n",
    "\n",
    "    # Feature Selection with Univariate Statistical Tests\n",
    "    # pouzijeme statisticke tety aby sme vybrali K najlepsich atributov ktore najviac vplyvaju na vysledok\n",
    "\n",
    "    UST_X = dataset_for_UST.drop('class', axis=1)\n",
    "    UST_y = dataset_for_UST['class']\n",
    "    # feature extraction\n",
    "    test = SelectKBest(score_func=f_classif, k=13)\n",
    "    fit = test.fit(UST_X, UST_y)\n",
    "    # summarize scores\n",
    "    #vidime ze niektore atributy maju miniaturny vplyv na vysledok priam az zanedbatelny tak ich vyhodime aby sme zrychlili vypocet modelu\n",
    "    print(fit.scores_)\n",
    "    features_columns = {}\n",
    "    for i in range(len(fit.scores_)):\n",
    "        features_columns[UST_X.columns[i]] = fit.scores_[i]\n",
    "    columns_by_rank = {k: features_columns[k] for k in sorted(features_columns, key=features_columns.get, reverse=True)}\n",
    "    columns_to_remove = list(columns_by_rank.keys())[13-(len(features_columns)):]\n",
    "    return dataset_for_UST.drop(columns_to_remove, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odstranenie outlierov\n",
    "zvolili sme metodu IRQ na detekciu outlierov a hodnoty ktore su outlieri nahradime krajnymi hodnotami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(dataset):\n",
    "    for column in dataset.columns:\n",
    "        Q1 = dataset[column].quantile(0.25)\n",
    "        Q3 = dataset[column].quantile(0.75)\n",
    "\n",
    "        IRQ = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - (1.5*IRQ)\n",
    "        upper_bound = Q3 + (1.5*IRQ)\n",
    "        if(dataset[column].dtype == 'int64'):\n",
    "            dataset[column] = dataset[column].apply(lambda val: math.floor(lower_bound) if val < lower_bound else math.floor(upper_bound) if val > upper_bound else val)\n",
    "        else:\n",
    "            dataset[column] = dataset[column].apply(lambda val: lower_bound if val < lower_bound else upper_bound if val > upper_bound else val)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transofrmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset):\n",
    "    scaler = RobustScaler()\n",
    "    scaled = scaler.fit_transform(dataset)\n",
    "    dataset = pd.DataFrame(scaled, columns=dataset.columns)\n",
    "\n",
    "    power = PowerTransformer(method='yeo-johnson', standardize=True) \n",
    "    data_trans = power.fit_transform(dataset)\n",
    "    return pd.DataFrame(data_trans, columns=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_data(personal_data, other_data)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataset.columns:\n",
    "    skewness = stats.skew(dataset[column])\n",
    "    tab = 20 - len(column)\n",
    "    if skewness <= 0.5 and skewness >= -0.5:\n",
    "        print(column,' ' * tab, 'symmetrical    ', skewness)\n",
    "    elif skewness < -0.5:\n",
    "        print(column,' ' * tab, 'negative skew  ', skewness)\n",
    "    else:\n",
    "        print(column,' ' * tab, 'positive skew  ', skewness)\n",
    "\n",
    "for column in dataset.columns:\n",
    "    kurtosis = stats.kurtosis(dataset[column])\n",
    "    tab = 20 - len(column)\n",
    "    if kurtosis <= 3.5 and kurtosis >= 2.5:\n",
    "        print(column,' ' * tab, 'symmetrical    ', skewness)\n",
    "    elif skewness < 2.5:\n",
    "        print(column,' ' * tab, 'in middle      ', skewness)\n",
    "    else:\n",
    "        print(column,' ' * tab, 'on outer       ', skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prieskumna analyza\n",
    "2 Datasety sme spojili do jedneho a odstranili sme duplicitne hodnoty a nulove hodnoty pre atribut class ktory je pre nas klucovy a nemozeme ho doplnit\n",
    "\n",
    "Dalej sme zredukovali pocet atributov na take ktore najviac kontribuuju k vysledku a cyhodili take ktore pravdepodobne vobec nekontribuuju.\n",
    "¨\n",
    "Data sme taktiez transformovali aby sa viac podobali normalnej distribucii a pochadzali z podobnych rozsahov hodnot.\n",
    "\n",
    "Tieto transformacie zmenili vyzor dat a v nasledujucej sekcii sa pozrieme na to ako sa zmenili distribucie a dalsie statisticke atributy dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['skewness_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['mean_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_glucose'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['mean_glucose'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['kurtosis_glucose'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['skewness_glucose'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozeme si vsimnut ze vsetky atributy maju velmi pdobne rozsahy ale velmi rozdielne priemery avsak podobne standardne distribucie. toto je vysledok transformacii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parova analyza a zistovanie zavislosti veku(age) a sklonu glukozy(skewness_glucose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(dataset['age'], line='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(dataset['skewness_glucose'], line='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=dataset['age'], y=dataset['skewness_glucose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = stats.pearsonr(dataset['age'], dataset['skewness_glucose']) \n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(dataset['age'], dataset['skewness_glucose'])[0, 1]\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre atributy age a skewness glucose sa nam corelacia takmer nezmenila no kovariancia je mensia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "stat, p =kruskal(dataset['age'], dataset['skewness_glucose'])\n",
    "\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['age'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['skewness_glucose'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parova analyza mean_glucose a mean_oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = stats.pearsonr(dataset['mean_oxygen'], dataset['mean_glucose']) \n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(dataset['mean_oxygen'], dataset['mean_glucose'])[0, 1]\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "stat, p =kruskal(dataset['mean_oxygen'], dataset['mean_glucose'])\n",
    "\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['mean_oxygen'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(dataset['mean_glucose'])\n",
    "print(shapiro_test)\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if shapiro_test.pvalue > alpha:\n",
    "    print('Normal distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Another distributions (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
