{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('IUA')",
   "metadata": {
    "interpreter": {
     "hash": "31e8c18851cc2616071ca7e5dcdbafd0634b8d1e2e632646433bf1c202497ec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats as sm_stats\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from math import sqrt\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_data = pd.read_csv(\"Dataset/personal_train.csv\")\n",
    "other_data = pd.read_csv(\"Dataset/other_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in personal_data:\n",
    "    del personal_data['Unnamed: 0']\n",
    "if 'Unnamed: 0' in other_data:\n",
    "    del other_data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_medical_name_dataset = other_data.dropna(subset=['medical_info']).drop_duplicates('name')\n",
    "unique_medical_name_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from 'medical_info' attribute\n",
    "medical_data_objects = []\n",
    "for index, record in unique_medical_name_dataset.iterrows():\n",
    "    if isinstance(record['medical_info'], float):\n",
    "        continue\n",
    "    medical_object = json.loads(record['medical_info'].replace(\"\\'\", '\\\"').replace(':\\\"',':').replace('\\\",',',').replace('\\\"}','}'))\n",
    "    medical_object['name'] = record['name']\n",
    "    medical_data_objects.append(medical_object)\n",
    "medical_info_dataset = pd.DataFrame(medical_data_objects)\n",
    "medical_info_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets to create single large dataset with usefull data so it's easier to create graphs and analysis\n",
    "merged_medical_info_dataset = unique_medical_name_dataset.merge(medical_info_dataset, on=['name'], how='outer').drop('medical_info', axis='columns')\n",
    "usefull_dataset = personal_data.merge(merged_medical_info_dataset, on=['name', 'address'], how='outer')\n",
    "usefull_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefull_dataset['occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "usefull_dataset['occupation'] = usefull_dataset['occupation'].map(lambda value: 'unknown' if type(value) is float else value.replace(' ','').replace('_', '-').replace('?', 'unknown'))\n",
    "usefull_dataset['occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_categories = usefull_dataset['occupation'].unique()"
   ]
  },
  {
   "source": [
    "NAN to values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = usefull_dataset\n",
    "test = test[test['class'].isnull()==False]\n",
    "test.replace('?',np.NaN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(numeric_features.tolist())\n",
    "print(categorical_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='median'))])\n",
    "cat = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan,strategy='most_frequent'))])\n",
    "full = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num',  num, numeric_features),\n",
    "        ('cat', cat, categorical_features)])\n",
    "\n",
    "# print(pd.DataFrame(full.fit_transform(test), columns=numeric_features.tolist() + categorical_features.tolist(),index=test.index))\n",
    "test2 = pd.DataFrame(full.fit_transform(test), columns=numeric_features.tolist() + categorical_features.tolist(),index=test.index)\n",
    "test2[test2.isnull().any(axis=1)]\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = usefull_dataset\n",
    "test3 = test[test['class'].isnull()==False]\n",
    "test3.replace('?',np.NaN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test3.drop('class', axis=1)\n",
    "y = test3['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.info(),X_test.info())\n",
    "print(y_train.count(),y_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_features = test3.select_dtypes(include=['int64', 'float64']).drop(['class'], axis=1).columns\n",
    "categorical_features = test3.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [4,5,6,7,8],\n",
    "    'classifier__criterion' :['gini', 'entropy']}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs= 1)\n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}