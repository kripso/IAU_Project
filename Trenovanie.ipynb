{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Zadanie\n",
    "## Autori: Rajcsanyi Roland, Setnicky Jakub Frantisek\n",
    "\n",
    "Fáza 3: Strojové učenie (max. 18b)\n",
    "Pri dátovej analýze nemusí byť naším cieľom získať len znalosti obsiahnuté v aktuálnych dátach, ale aj natrénovať model, ktorý bude schopný robiť rozumné predikcie pre nové pozorovania pomocou techniky strojového učenia.\n",
    "\n",
    "V tomto projekte sa zameriame na rozhodovacie stromy vzhľadom na ich jednoduchú interpretovateľnosť. V tejto fáze použite všetky data, ktoré ste dostali. Oddemonštrujete znovupoužiteľnosť vami realizovaného predspracovania.\n",
    "\n",
    "Predspracovanie validačného datasetu Vami realizovaným postupom predspracovania a opis prípadných zmien (3b).\n",
    "\n",
    "Spustite postup predspracovania realizovaný v predchádzajúcej fáze nad novým datasetom. Nový dataset bude mať rovnakú štruktúru ako váš pôvodný.\n",
    "Ak si spustenie predspracovania vyžiada zmeny v kóde, opíšte ich. Zabezpečíte aby postup predspracovanie funguje rovnako na obidve datasety.\n",
    "Manuálne vytvorenie a vyhodnotenie rozhodovacích pravidiel pre klasifikáciu (3b).\n",
    "\n",
    "Vyskúšajte jednoduché pravidlá (1R) zahŕňajúce jeden atribút pre CART, ale aj komplikovanejšie zahŕňajúce viacero atribútov (ich kombinácie).\n",
    "Pravidlá by v tomto kroku mali byť vytvorené manuálne na základe pozorovaných závislostí v dátach.\n",
    "Pravidlá (manuálne vytvorené klasifikátory) vyhodnoťte pomocou metrík accuracy, precision a recall.\n",
    "Natrénovanie a vyhodnotenie klasifikátora s využitím rozhodovacích stromov (6b).\n",
    "\n",
    "Na trénovanie využite stromový algoritmus (resp. algoritmy) dostupný/é v scikit-learn.\n",
    "Vizualizujte natrénované pravidlá.\n",
    "Vyhodnoťte natrénovaný rozhodovací strom pomocou metrík accuracy, precision a recall\n",
    "Porovnajte natrénovaný klasifikátor s Vašimi manuálne vytvorenými pravidlami z druhého kroku.\n",
    "Optimalizácia hyperparametrov (4b).\n",
    "\n",
    "Preskúmajte hyperparametre Vášho zvoleného klasifikačného algoritmu a vyskúšajte ich rôzne nastavenie tak, aby ste minimalizovali overfitting (preučenie) a optimalizovali výsledok. Vysvetlite, čo jednotlivé hyperparametre robia.\n",
    "Pri nastavovaní hyperparametrov algoritmu využite 10-násobnú krížovú validáciu na trénovacej množine.\n",
    "Vyhodnotenie vplyvu zvolenej stratégie riešenia na správnosť klasifikácie (2b).\n",
    "\n",
    "Zistite, či použitie zvolených stratégií riešenia chýbajúcich hodnôt vplýva na accuracy klasifikácie.\n",
    "Ktorá stratégia sa ukázala ako vhodnejšia pre daný problém?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import math"
   ]
  },
  {
   "source": [
    "nacitanie datasetov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_SI_dataset = pd.read_csv(\"Dataset/valid_SI_processed_dataset.csv\")\n",
    "train_SI_dataset = pd.read_csv(\"Dataset/train_SI_processed_dataset.csv\")\n",
    "valid_KNI_dataset = pd.read_csv(\"Dataset/valid_KNI_processed_dataset.csv\")\n",
    "train_KNI_dataset = pd.read_csv(\"Dataset/train_KNI_processed_dataset.csv\")\n",
    "non_processed_valid = pd.read_csv(\"Dataset/other_valid.csv\")\n",
    "non_processed_train = pd.read_csv(\"Dataset/other_train.csv\")"
   ]
  },
  {
   "source": [
    "transofrmacia atributu class z float na int kvoli neskorsim metodam ktore by nepracovali spravne s float pre class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_SI_dataset['class'] = valid_SI_dataset['class'].apply(lambda row: math.floor(row))\n",
    "train_SI_dataset['class'] = train_SI_dataset['class'].apply(lambda row: math.floor(row))\n",
    "valid_KNI_dataset['class'] = valid_KNI_dataset['class'].apply(lambda row: math.floor(row))\n",
    "train_KNI_dataset['class'] = train_KNI_dataset['class'].apply(lambda row: math.floor(row))"
   ]
  },
  {
   "source": [
    "# Porovnanie predspracovania\n",
    "\n",
    "Nizsie su vypisane atributy predspracovanych validacnych ako aj trenovacich datasetov. Vidime ze predspracovanie vybralo rovnake atributy z oboch datasetov. Pocet zaznamov bol v datasetoch rozdielny a tak je rozdielny aj pocet zaznamov po spracovani. Rozdiel v pocetnosti zaznamov sposobil ze sa rozdelenia atributov mierne odlisuju medzi datasetmi ked zoberieme do uvahy pomer rozdeleni pred a po spracovani avsak tento rozdiel ja marginalny. To ze sa predspracovany trenovaci a validacny dataset velmi podobaju nam overilo ze predspracovanie je mozne vhodne pouzit na oba datassety a nemusime sa obavat roznych vysledkov pri pouziti hociktoreho z datasetov. Dalej vidime ze niektore atributy sa lisia v zalezitosti od toho akym sposobom boli nahradzovane NaN hodnoty a vyhodnotenie tychto dvoch rozlicnych strategii bude urobene neskor. vidime vsak ze zmena medzi strategiami je konzistentna pri validacnom aj trenovacom datasete."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_SI_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SI_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_KNI_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_KNI_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_processed_valid['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_processed_train['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_SI_dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_SI_dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_KNI_dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_KNI_dataset['kurtosis_oxygen'].describe()"
   ]
  },
  {
   "source": [
    "# Detekcia Overfittingu\n",
    "\n",
    "V tejto casti vyskusame ci nie je nas model pretrenovany a pokusime sa ho zlepsit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SI_valid = valid_SI_dataset.drop(\"class\", axis=1)\n",
    "y_SI_valid = valid_SI_dataset[\"class\"]\n",
    "X_KNI_valid = valid_KNI_dataset.drop(\"class\", axis=1)\n",
    "y_KNI_valid = valid_KNI_dataset[\"class\"]\n",
    "X_SI_train = train_SI_dataset.drop(\"class\", axis=1)\n",
    "y_SI_train = train_SI_dataset[\"class\"]\n",
    "X_KNI_train = train_KNI_dataset.drop(\"class\", axis=1)\n",
    "y_KNI_train = train_KNI_dataset[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SI_valid_train, X_SI_valid_test, y_SI_valid_train, y_SI_valid_test = train_test_split(X_SI_valid, y_SI_valid, test_size=0.33, random_state=40)\n",
    "clf_SI_valid = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf_SI_valid.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "\n",
    "y_SI_valid_pred = clf_SI_valid.predict(X_SI_valid_test)\n",
    "cm_SI_valid = metrics.confusion_matrix(y_SI_valid_test, y_SI_valid_pred)\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Logistic regression Accuracy:', metrics.accuracy_score(y_SI_valid_test, y_SI_valid_pred))\n",
    "print(\"Logistic regresion:\")\n",
    "print(pd.DataFrame(cm_SI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "y_SI_valid_pred_proba = clf_SI_valid.predict_proba(X_SI_valid_test)[::,1]\n",
    "fpr_SI_valid, tpr_SI_valid, threshold_SI_valid = metrics.roc_curve(y_SI_valid_test, y_SI_valid_pred_proba)\n",
    "\n",
    "auc_SI_valid = metrics.roc_auc_score(y_SI_valid_test, y_SI_valid_pred_proba)\n",
    "print('FPR', fpr_SI_valid, \n",
    "      '\\nTPR', tpr_SI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_SI_valid)\n",
    "\n",
    "plt.plot(fpr_SI_valid, tpr_SI_valid, label=\"data 1, auc=\"+str(auc_SI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SI_valid = []\n",
    "estimators_SI_valid = []\n",
    "for i in range(1, X_SI_valid_train.shape[1] + 1):  \n",
    "    row_SI_valid = {'model_complexity': i}\n",
    "    \n",
    "    # Vytvoríme rozhodovací strom\n",
    "    # strom s maximalnou hlbkou 1-pocet atributov,  simulujeme tak zlozitost modelu\n",
    "    clf_SI_valid = DecisionTreeClassifier(max_depth = i) \n",
    "    \n",
    "    # natrenovanie modelu a predikovanie na trenovacej sade\n",
    "    pred_SI_valid = clf_SI_valid.fit(X_SI_valid_train, y_SI_valid_train).predict(X_SI_valid_train) \n",
    "    \n",
    "    # chyba na trenovacej sade\n",
    "    row_SI_valid['train'] = 1-accuracy_score(y_SI_valid_train, pred_SI_valid) \n",
    "    \n",
    "    # predickcia\n",
    "    pred_SI_valid = clf_SI_valid.predict(X_SI_valid_test)\n",
    "    \n",
    "    # chyba na testovacej sade\n",
    "    row_SI_valid['test'] = 1-accuracy_score(y_SI_valid_test, pred_SI_valid) \n",
    "    results_SI_valid.append(row_SI_valid)\n",
    "    estimators_SI_valid.append(clf_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_SI_valid = pd.DataFrame(results_SI_valid)\n",
    "complexity_df_SI_valid.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_SI_valid.plot(x='model_complexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KNI_valid_train, X_KNI_valid_test, y_KNI_valid_train, y_KNI_valid_test = train_test_split(X_KNI_valid, y_KNI_valid, test_size=0.33, random_state=40)\n",
    "clf_KNI_valid = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf_KNI_valid.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "\n",
    "y_KNI_valid_pred = clf_KNI_valid.predict(X_KNI_valid_test)\n",
    "cm_KNI_valid = metrics.confusion_matrix(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Logistic regression Accuracy:', metrics.accuracy_score(y_KNI_valid_test, y_KNI_valid_pred))\n",
    "print(\"Logistic regresion:\")\n",
    "print(pd.DataFrame(cm_KNI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "y_KNI_valid_pred_proba = clf_KNI_valid.predict_proba(X_KNI_valid_test)[::,1]\n",
    "fpr_KNI_valid, tpr_KNI_valid, threshold_KNI_valid = metrics.roc_curve(y_KNI_valid_test, y_KNI_valid_pred_proba)\n",
    "\n",
    "auc_KNI_valid = metrics.roc_auc_score(y_KNI_valid_test, y_KNI_valid_pred_proba)\n",
    "print('FPR', fpr_KNI_valid, \n",
    "      '\\nTPR', tpr_KNI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_KNI_valid)\n",
    "\n",
    "plt.plot(fpr_KNI_valid, tpr_KNI_valid, label=\"data 1, auc=\"+str(auc_KNI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNI_valid = []\n",
    "estimators_KNI_valid = []\n",
    "for i in range(1, X_KNI_valid_train.shape[1] + 1):  \n",
    "    row_KNI_valid = {'model_complexity': i}\n",
    "    \n",
    "    # Vytvoríme rozhodovací strom\n",
    "    # strom s maximalnou hlbkou 1-pocet atributov,  simulujeme tak zlozitost modelu\n",
    "    clf_KNI_valid = DecisionTreeClassifier(max_depth = i) \n",
    "    \n",
    "    # natrenovanie modelu a predikovanie na trenovacej sade\n",
    "    pred_KNI_valid = clf_KNI_valid.fit(X_KNI_valid_train, y_KNI_valid_train).predict(X_KNI_valid_train) \n",
    "    \n",
    "    # chyba na trenovacej sade\n",
    "    row_KNI_valid['train'] = 1-accuracy_score(y_KNI_valid_train, pred_KNI_valid) \n",
    "    \n",
    "    # predickcia\n",
    "    pred_KNI_valid = clf_KNI_valid.predict(X_KNI_valid_test)\n",
    "    \n",
    "    # chyba na testovacej sade\n",
    "    row_KNI_valid['test'] = 1-accuracy_score(y_KNI_valid_test, pred_KNI_valid) \n",
    "    results_KNI_valid.append(row_KNI_valid)\n",
    "    estimators_KNI_valid.append(clf_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_KNI_valid = pd.DataFrame(results_KNI_valid)\n",
    "complexity_df_KNI_valid.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_KNI_valid.plot(x='model_complexity')"
   ]
  },
  {
   "source": [
    "Mozeme vidiet ze validacne datasety s oboma strategiami NaN maju zvysujucu sa chybu so zvysujucou sa komplexitou co znamena ze model bol pretrenovany a natrenovali sa data a nie spojenia medzi datami. skusime aplikovat Redukciu atributov pomocou Sequential forward selection na vybratie k najlepsich atributov pokym nedosiahneme klesajucu chybu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection(sfs) for SI_valid\n",
    "sfs = SFS(LinearRegression(),\n",
    "          k_features=3,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "sfs.fit(X_SI_valid, y_SI_valid)\n",
    "X_SI_valid = X_SI_valid.filter(sfs.k_feature_names_)\n",
    "X_SI_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SI_valid_train, X_SI_valid_test, y_SI_valid_train, y_SI_valid_test = train_test_split(X_SI_valid, y_SI_valid, test_size=0.33, random_state=40)\n",
    "clf_SI_valid = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf_SI_valid.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "\n",
    "y_SI_valid_pred = clf_SI_valid.predict(X_SI_valid_test)\n",
    "cm_SI_valid = metrics.confusion_matrix(y_SI_valid_test, y_SI_valid_pred)\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Logistic regression Accuracy:', metrics.accuracy_score(y_SI_valid_test, y_SI_valid_pred))\n",
    "print(\"Logistic regresion:\")\n",
    "print(pd.DataFrame(cm_SI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "y_SI_valid_pred_proba = clf_SI_valid.predict_proba(X_SI_valid_test)[::,1]\n",
    "fpr_SI_valid, tpr_SI_valid, threshold_SI_valid = metrics.roc_curve(y_SI_valid_test, y_SI_valid_pred_proba)\n",
    "\n",
    "auc_SI_valid = metrics.roc_auc_score(y_SI_valid_test, y_SI_valid_pred_proba)\n",
    "print('FPR', fpr_SI_valid, \n",
    "      '\\nTPR', tpr_SI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_SI_valid)\n",
    "\n",
    "plt.plot(fpr_SI_valid, tpr_SI_valid, label=\"data 1, auc=\"+str(auc_SI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SI_valid = []\n",
    "estimators_SI_valid = []\n",
    "for i in range(1, X_SI_valid_train.shape[1] + 1):  \n",
    "    row_SI_valid = {'model_complexity': i}\n",
    "    \n",
    "    # Vytvoríme rozhodovací strom\n",
    "    # strom s maximalnou hlbkou 1-pocet atributov,  simulujeme tak zlozitost modelu\n",
    "    clf_SI_valid = DecisionTreeClassifier(max_depth = i) \n",
    "    \n",
    "    # natrenovanie modelu a predikovanie na trenovacej sade\n",
    "    pred_SI_valid = clf_SI_valid.fit(X_SI_valid_train, y_SI_valid_train).predict(X_SI_valid_train) \n",
    "    \n",
    "    # chyba na trenovacej sade\n",
    "    row_SI_valid['train'] = 1-accuracy_score(y_SI_valid_train, pred_SI_valid) \n",
    "    \n",
    "    # predickcia\n",
    "    pred_SI_valid = clf_SI_valid.predict(X_SI_valid_test)\n",
    "    \n",
    "    # chyba na testovacej sade\n",
    "    row_SI_valid['test'] = 1-accuracy_score(y_SI_valid_test, pred_SI_valid) \n",
    "    results_SI_valid.append(row_SI_valid)\n",
    "    estimators_SI_valid.append(clf_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_SI_valid = pd.DataFrame(results_SI_valid)\n",
    "complexity_df_SI_valid.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_SI_valid.plot(x='model_complexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection(sfs) for KNI_valid\n",
    "sfs = SFS(LinearRegression(),\n",
    "          k_features=4,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring = 'r2',\n",
    "          cv = 0)\n",
    "sfs.fit(X_KNI_valid, y_KNI_valid)\n",
    "X_KNI_valid = X_KNI_valid.filter(sfs.k_feature_names_)\n",
    "X_KNI_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KNI_valid_train, X_KNI_valid_test, y_KNI_valid_train, y_KNI_valid_test = train_test_split(X_KNI_valid, y_KNI_valid, test_size=0.33, random_state=40)\n",
    "clf_KNI_valid = LogisticRegression(penalty='l2', \n",
    "                         C=0.1, \n",
    "                         max_iter=5000)\n",
    "clf_KNI_valid.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "\n",
    "y_KNI_valid_pred = clf_KNI_valid.predict(X_KNI_valid_test)\n",
    "cm_KNI_valid = metrics.confusion_matrix(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Logistic regression Accuracy:', metrics.accuracy_score(y_KNI_valid_test, y_KNI_valid_pred))\n",
    "print(\"Logistic regresion:\")\n",
    "print(pd.DataFrame(cm_KNI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "y_KNI_valid_pred_proba = clf_KNI_valid.predict_proba(X_KNI_valid_test)[::,1]\n",
    "fpr_KNI_valid, tpr_KNI_valid, threshold_KNI_valid = metrics.roc_curve(y_KNI_valid_test, y_KNI_valid_pred_proba)\n",
    "\n",
    "auc_KNI_valid = metrics.roc_auc_score(y_KNI_valid_test, y_KNI_valid_pred_proba)\n",
    "print('FPR', fpr_KNI_valid, \n",
    "      '\\nTPR', tpr_KNI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_KNI_valid)\n",
    "\n",
    "plt.plot(fpr_KNI_valid, tpr_KNI_valid, label=\"data 1, auc=\"+str(auc_KNI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNI_valid = []\n",
    "estimators_KNI_valid = []\n",
    "for i in range(1, X_KNI_valid_train.shape[1] + 1):  \n",
    "    row_KNI_valid = {'model_complexity': i}\n",
    "    \n",
    "    # Vytvoríme rozhodovací strom\n",
    "    # strom s maximalnou hlbkou 1-pocet atributov,  simulujeme tak zlozitost modelu\n",
    "    clf_KNI_valid = DecisionTreeClassifier(max_depth = i) \n",
    "    \n",
    "    # natrenovanie modelu a predikovanie na trenovacej sade\n",
    "    pred_KNI_valid = clf_KNI_valid.fit(X_KNI_valid_train, y_KNI_valid_train).predict(X_KNI_valid_train) \n",
    "    \n",
    "    # chyba na trenovacej sade\n",
    "    row_KNI_valid['train'] = 1-accuracy_score(y_KNI_valid_train, pred_KNI_valid) \n",
    "    \n",
    "    # predickcia\n",
    "    pred_KNI_valid = clf_KNI_valid.predict(X_KNI_valid_test)\n",
    "    \n",
    "    # chyba na testovacej sade\n",
    "    row_KNI_valid['test'] = 1-accuracy_score(y_KNI_valid_test, pred_KNI_valid) \n",
    "    results_KNI_valid.append(row_KNI_valid)\n",
    "    estimators_KNI_valid.append(clf_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_KNI_valid = pd.DataFrame(results_KNI_valid)\n",
    "complexity_df_KNI_valid.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_df_KNI_valid.plot(x='model_complexity')"
   ]
  },
  {
   "source": [
    "Vyssie vidime ze pri datasete SI sme museli atributy zredukovat az na 3 a pri KNI na 4 atributy. presnost modelu sa mierne znizila pri porovnani s predoslym to vsak moze byt dosledkom zle zvolenych hyperparametrov ktore budeme optimalizovat nizsie"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizacia stromu pre SI_valid\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "graph = Source(export_graphviz(estimators_SI_valid[-1], # najzlozitejsi model\n",
    "                               out_file=None,\n",
    "                               feature_names=X_SI_valid.columns,\n",
    "                               class_names=[\"positive\", \"negative\"],\n",
    "                               filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "from IPython.display import HTML # toto je tu len pre to aby sa mi obrazok zmestil na obrazovku\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizacia stromu pre KNI_valid\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "graph = Source(export_graphviz(estimators_KNI_valid[-1], # najzlozitejsi model\n",
    "                               out_file=None,\n",
    "                               feature_names=X_KNI_valid.columns,\n",
    "                               class_names=[\"positive\", \"negative\"],\n",
    "                               filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "from IPython.display import HTML # toto je tu len pre to aby sa mi obrazok zmestil na obrazovku\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "source": [
    "# Vytvorenie rozhodovacich stromov s vlastnymi pravidlami\n",
    "\n",
    "na Zaklade pozorovani zavislosti medzi datami vytvorime rozhodovacie stromy s vlastnymi pravidlami a porovname vysledky so stromami z kniznice scikit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_KNI_valid[X_KNI_valid.columns[0]], y=y_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_KNI_valid[X_KNI_valid.columns[1]], y=y_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_KNI_valid[X_KNI_valid.columns[2]], y=y_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_KNI_valid[X_KNI_valid.columns[3]], y=y_KNI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_SI_valid[X_SI_valid.columns[0]], y=y_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_SI_valid[X_SI_valid.columns[1]], y=y_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_SI_valid[X_SI_valid.columns[2]], y=y_SI_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A decision tree node.\n",
    "    code from : https://github.com/joachimvalente/decision-tree-cart/blob/master/tree.py\n",
    "    used for the printing logic and basic structure of node\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def debug(self, feature_names, class_names, show_details):\n",
    "        \"\"\"Print an ASCII visualization of the tree.\"\"\"\n",
    "        lines, _, _, _ = self._debug_aux(\n",
    "            feature_names, class_names, show_details, root=True\n",
    "        )\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "    def _debug_aux(self, feature_names, class_names, show_details, root=False):\n",
    "        # See https://stackoverflow.com/a/54074933/1143396 for similar code.\n",
    "        is_leaf = not self.right\n",
    "        if is_leaf:\n",
    "            lines = [class_names[self.predicted_class]]\n",
    "        else:\n",
    "            lines = [\n",
    "                \"{} < {:.2f}\".format(feature_names[self.feature_index], self.threshold)\n",
    "            ]\n",
    "        if show_details:\n",
    "            lines += [\n",
    "                \"gini = {:.2f}\".format(self.gini),\n",
    "                \"samples = {}\".format(self.num_samples),\n",
    "                str(self.num_samples_per_class),\n",
    "            ]\n",
    "        width = max(len(line) for line in lines)\n",
    "        height = len(lines)\n",
    "        if is_leaf:\n",
    "            lines = [\"║ {:^{width}} ║\".format(line, width=width) for line in lines]\n",
    "            lines.insert(0, \"╔\" + \"═\" * (width + 2) + \"╗\")\n",
    "            lines.append(\"╚\" + \"═\" * (width + 2) + \"╝\")\n",
    "        else:\n",
    "            lines = [\"│ {:^{width}} │\".format(line, width=width) for line in lines]\n",
    "            lines.insert(0, \"┌\" + \"─\" * (width + 2) + \"┐\")\n",
    "            lines.append(\"└\" + \"─\" * (width + 2) + \"┘\")\n",
    "            lines[-2] = \"┤\" + lines[-2][1:-1] + \"├\"\n",
    "        width += 4  # for padding\n",
    "\n",
    "        if is_leaf:\n",
    "            middle = width // 2\n",
    "            lines[0] = lines[0][:middle] + \"╧\" + lines[0][middle + 1 :]\n",
    "            return lines, width, height, middle\n",
    "\n",
    "        # If not a leaf, must have two children.\n",
    "        left, n, p, x = self.left._debug_aux(feature_names, class_names, show_details)\n",
    "        right, m, q, y = self.right._debug_aux(feature_names, class_names, show_details)\n",
    "        top_lines = [n * \" \" + line + m * \" \" for line in lines[:-2]]\n",
    "        # fmt: off\n",
    "        middle_line = x * \" \" + \"┌\" + (n - x - 1) * \"─\" + lines[-2] + y * \"─\" + \"┐\" + (m - y - 1) * \" \"\n",
    "        bottom_line = x * \" \" + \"│\" + (n - x - 1) * \" \" + lines[-1] + y * \" \" + \"│\" + (m - y - 1) * \" \"\n",
    "        # fmt: on\n",
    "        if p < q:\n",
    "            left += [n * \" \"] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * \" \"] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = (\n",
    "            top_lines\n",
    "            + [middle_line, bottom_line]\n",
    "            + [a + width * \" \" + b for a, b in zipped_lines]\n",
    "        )\n",
    "        middle = n + width // 2\n",
    "        if not root:\n",
    "            lines[0] = lines[0][:middle] + \"┴\" + lines[0][middle + 1 :]\n",
    "        return lines, n + m + width, max(p, q) + 2 + len(top_lines), middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnTreeImplementation:\n",
    "    def __init__(self, max_depth=None, min_split=1, min_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_split = min_split\n",
    "        self.min_leaf = min_leaf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree classifier.\"\"\"\n",
    "        self.n_of_classes_ = len(set(y))  # classes are assumed to go from 0 to n-1\n",
    "        self.n_of_features_ = len(X.columns)\n",
    "        self.tree_full = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\"\"\"\n",
    "        return [self._predict(inputs, X.columns) for index, inputs in X.iterrows()]\n",
    "\n",
    "    def debug(self, feature_names, class_names, show_details=True):\n",
    "        \"\"\"Print ASCII visualization of decision tree.\"\"\"\n",
    "        self.tree_full.debug(feature_names, class_names, show_details)\n",
    "\n",
    "    def _gini(self, y):\n",
    "        m = y.size\n",
    "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in range(self.n_of_classes_))\n",
    "\n",
    "    def _split(self, X, y):\n",
    "        # Need at least two elements to split a node.\n",
    "        m = y.size\n",
    "        if m <= self.min_split:\n",
    "            return None, None\n",
    "\n",
    "        # Count of each class in the current node.\n",
    "        num_of_parent = [np.sum(y == c) for c in range(self.n_of_classes_)]\n",
    "\n",
    "        # Gini of current node.\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_of_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "\n",
    "        # Loop through all features.\n",
    "        for idx in range(self.n_of_features_):\n",
    "            # Sort data along selected feature.\n",
    "            thresholds, classes = zip(*sorted(zip(X.iloc[:, idx], y)))\n",
    "            num_left = [0] * self.n_of_classes_\n",
    "            num_right = num_of_parent.copy()\n",
    "            for i in range(1, m):  # possible split positions\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_of_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_of_classes_)\n",
    "                )\n",
    "\n",
    "                # The Gini impurity of a split is the weighted average of the Gini\n",
    "                # impurity of the children.\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2 # middle point\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"Build a decision tree by recursively finding the best split.\"\"\"\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_of_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(\n",
    "            gini=self._gini(y),\n",
    "            num_samples=y.size,\n",
    "            num_samples_per_class=num_samples_per_class,\n",
    "            predicted_class=predicted_class,\n",
    "        )\n",
    "\n",
    "        # Split recursively until maximum depth is reached.\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X.query(f'{X.columns[idx]} < {thr}').index.tolist()\n",
    "                if len(X.index) - len(indices_left) < self.min_leaf:\n",
    "                    indices_left = X.index.tolist()\n",
    "                elif len(indices_left) < self.min_leaf:\n",
    "                    indeces_left = []\n",
    "                X_left, y_left = X.loc[indices_left], y.loc[indices_left]\n",
    "                X_right, y_right = X.drop(indices_left, axis=0), y.drop(indices_left, axis=0)\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs, features):\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        node = self.tree_full\n",
    "        while node.left:\n",
    "            if inputs[features[node.feature_index]] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class"
   ]
  },
  {
   "source": [
    "Above is an implementation of decision tree to which we can set depth, minimal split parameter and minimal leaf parameter\n",
    "if there are less items in node than min_split the node will no longer split and its result will be given\n",
    "if there would be less items in any of child nodes than min_leaf all items will go to the other leaf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_KNI_valid_train, X_KNI_valid_test, y_KNI_valid_train, y_KNI_valid_test = train_test_split(X_KNI_valid, y_KNI_valid, test_size=0.33, random_state=40)\n",
    "# Fit data.\n",
    "clf_one_KNI = OwnTreeImplementation(max_depth=1, min_split=1, min_leaf=1)\n",
    "clf_one_KNI.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "y_KNI_valid_pred = clf_one_KNI.predict(X_KNI_valid_test)\n",
    "cm_KNI_valid = metrics.confusion_matrix(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Own tree Accuracy:', metrics.accuracy_score(y_KNI_valid_test, y_KNI_valid_pred))\n",
    "print(\"Own tree:\")\n",
    "print(pd.DataFrame(cm_KNI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "fpr_KNI_valid, tpr_KNI_valid, threshold_KNI_valid = metrics.roc_curve(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "\n",
    "auc_KNI_valid = metrics.roc_auc_score(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "print('FPR', fpr_KNI_valid, \n",
    "      '\\nTPR', tpr_KNI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_KNI_valid)\n",
    "\n",
    "plt.plot(fpr_KNI_valid, tpr_KNI_valid, label=\"data 1, auc=\"+str(auc_KNI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "clf_one_KNI.debug(X_KNI_valid.columns, ['negative', 'positive']) # output can be found in OwnKNITreeDepth1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KNI_valid_train, X_KNI_valid_test, y_KNI_valid_train, y_KNI_valid_test = train_test_split(X_KNI_valid, y_KNI_valid, test_size=0.33, random_state=40)\n",
    "# Fit data.\n",
    "clf_five_KNI = OwnTreeImplementation(max_depth=5, min_split=1, min_leaf=1)\n",
    "clf_five_KNI.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "y_KNI_valid_pred = clf_five_KNI.predict(X_KNI_valid_test)\n",
    "cm_KNI_valid = metrics.confusion_matrix(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('Own tree Accuracy:', metrics.accuracy_score(y_KNI_valid_test, y_KNI_valid_pred))\n",
    "print(\"Own tree:\")\n",
    "print(pd.DataFrame(cm_KNI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_KNI_valid, fp_KNI_valid, fn_KNI_valid, tp_KNI_valid = cm_KNI_valid.ravel()\n",
    "print(f'True Positives: {tp_KNI_valid}', \n",
    "      f'False Positives: {fp_KNI_valid}', \n",
    "      f'True Negatives: {tn_KNI_valid}',\n",
    "      f'False Negatives: {fn_KNI_valid}', '\\n')\n",
    "\n",
    "\n",
    "fpr_KNI_valid, tpr_KNI_valid, threshold_KNI_valid = metrics.roc_curve(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "\n",
    "auc_KNI_valid = metrics.roc_auc_score(y_KNI_valid_test, y_KNI_valid_pred)\n",
    "print('FPR', fpr_KNI_valid, \n",
    "      '\\nTPR', tpr_KNI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_KNI_valid)\n",
    "\n",
    "plt.plot(fpr_KNI_valid, tpr_KNI_valid, label=\"data 1, auc=\"+str(auc_KNI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "clf_five_KNI.debug(X_KNI_valid.columns, ['negative', 'positive']) # output can be found in OwnKNITreeDepth5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SI_valid_train, X_SI_valid_test, y_SI_valid_train, y_SI_valid_test = train_test_split(X_SI_valid, y_SI_valid, test_size=0.33, random_state=40)\n",
    "# Fit data.\n",
    "clf_one_SI = OwnTreeImplementation(max_depth=1, min_split=1, min_leaf=1)\n",
    "clf_one_SI.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "\n",
    "y_SI_valid_pred = clf_one_SI.predict(X_SI_valid_test)\n",
    "cm_SI_valid = metrics.confusion_matrix(y_SI_valid_test, y_SI_valid_pred)\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('own tree Accuracy:', metrics.accuracy_score(y_SI_valid_test, y_SI_valid_pred))\n",
    "print(\"own tree regresion:\")\n",
    "print(pd.DataFrame(cm_SI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "fpr_SI_valid, tpr_SI_valid, threshold_SI_valid = metrics.roc_curve(y_SI_valid_test, y_SI_valid_pred)\n",
    "\n",
    "auc_SI_valid = metrics.roc_auc_score(y_SI_valid_test, y_SI_valid_pred)\n",
    "print('FPR', fpr_SI_valid, \n",
    "      '\\nTPR', tpr_SI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_SI_valid)\n",
    "\n",
    "plt.plot(fpr_SI_valid, tpr_SI_valid, label=\"data 1, auc=\"+str(auc_SI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "clf_one_SI.debug(X_KNI_valid.columns, ['negative', 'positive']) # output can be found in OwnSITreeDepth1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SI_valid_train, X_SI_valid_test, y_SI_valid_train, y_SI_valid_test = train_test_split(X_SI_valid, y_SI_valid, test_size=0.33, random_state=40)\n",
    "# Fit data.\n",
    "clf_five_SI = OwnTreeImplementation(max_depth=5, min_split=1, min_leaf=1)\n",
    "clf_five_SI.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "\n",
    "y_SI_valid_pred = clf_five_SI.predict(X_SI_valid_test)\n",
    "cm_SI_valid = metrics.confusion_matrix(y_SI_valid_test, y_SI_valid_pred)\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "\n",
    "print('own tree Accuracy:', metrics.accuracy_score(y_SI_valid_test, y_SI_valid_pred))\n",
    "print(\"own tree regresion:\")\n",
    "print(pd.DataFrame(cm_SI_valid, \n",
    "                   columns=['Predicted Negative', 'Predicted Positive'], \n",
    "                   index=['Actual Negative', 'Actual Positive']), '\\n')\n",
    "tn_SI_valid, fp_SI_valid, fn_SI_valid, tp_SI_valid = cm_SI_valid.ravel()\n",
    "print(f'True Positives: {tp_SI_valid}', \n",
    "      f'False Positives: {fp_SI_valid}', \n",
    "      f'True Negatives: {tn_SI_valid}',\n",
    "      f'False Negatives: {fn_SI_valid}', '\\n')\n",
    "\n",
    "fpr_SI_valid, tpr_SI_valid, threshold_SI_valid = metrics.roc_curve(y_SI_valid_test, y_SI_valid_pred)\n",
    "\n",
    "auc_SI_valid = metrics.roc_auc_score(y_SI_valid_test, y_SI_valid_pred)\n",
    "print('FPR', fpr_SI_valid, \n",
    "      '\\nTPR', tpr_SI_valid, \n",
    "      '\\nthreshold', \n",
    "      '\\nthreshold',  \n",
    "      '\\nROC-AUC', auc_SI_valid)\n",
    "\n",
    "plt.plot(fpr_SI_valid, tpr_SI_valid, label=\"data 1, auc=\"+str(auc_SI_valid) )\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "clf_five_SI.debug(X_KNI_valid.columns, ['negative', 'positive']) # output can be found in OwnSITreeDepth5.txt"
   ]
  },
  {
   "source": [
    "# Optimalizacia hyper parametrov\n",
    "\n",
    "Hyper parametre mozu mat vplyc na vysledok natrenovaneho modelu na to aby sme dosiahli co najlepsie vysledky skusime nastavit rozne kombinacie hyperparametrov pomocou funkcii randomizedSearchCV a nasledne GridSearchCV ktore vyskusalu a cross validuju rozne hyperparametre a vyberu taky ktory ma najlepsie vysledky aby sme nasledne mohli natrenovat model s tymito parametrami"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "\n",
    "bestAcuracyClassifier = [None,0]\n",
    "X_SI_valid_train, X_SI_valid_test, y_SI_valid_train, y_SI_valid_test = train_test_split(X_SI_valid, y_SI_valid, test_size=0.33, random_state=40)\n",
    "X_KNI_valid_train, X_KNI_valid_test, y_KNI_valid_train, y_KNI_valid_test = train_test_split(X_KNI_valid, y_KNI_valid, test_size=0.33, random_state=40)\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100, max_features=3),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=10),\n",
    "    BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=10)\n",
    "]\n",
    "print('SI dataset:')\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "    pred = pipe.predict(X_SI_valid_test)\n",
    "\n",
    "    falseP = 0\n",
    "    falseN = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != y_SI_valid_test.tolist()[i]:\n",
    "            if pred[i] == 1:\n",
    "                falseP += 1\n",
    "            else: \n",
    "                falseN += 1\n",
    "\n",
    "    if  accuracy_score(y_KNI_valid_test, pred) > bestAcuracyClassifier[1]:\n",
    "        bestAcuracyClassifier[0]= classifier\n",
    "        bestAcuracyClassifier[1] = accuracy_score(y_SI_valid_test, pred)\n",
    "\n",
    "    print(classifier)\n",
    "    print()\n",
    "    print(\"accuracy:\", accuracy_score(y_SI_valid_test, pred))\n",
    "    print(\"precision:\", precision_score(y_SI_valid_test, pred, average='micro'))\n",
    "    print(\"recall:\", recall_score(y_SI_valid_test, pred, average='macro'))\n",
    "    print()\n",
    "    print(classification_report(y_SI_valid_test, pred))\n",
    "    print()\n",
    "    print(\"False Positive: {}\".format(falseP))\n",
    "    print(\"False Negative: {}\".format(falseN))\n",
    "    print('-----------------------------------------------------------------------------------------')\n",
    "print(bestAcuracyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNI dataset:')\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=100, max_features=len(X_KNI_valid_test.columns)),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=10),\n",
    "    BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=10)\n",
    "]\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "    pred = pipe.predict(X_KNI_valid_test)\n",
    "\n",
    "    falseP = 0\n",
    "    falseN = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] != y_KNI_valid_test.tolist()[i]:\n",
    "            if pred[i] == 1:\n",
    "                falseP += 1\n",
    "            else: \n",
    "                falseN += 1\n",
    "\n",
    "    if  accuracy_score(y_KNI_valid_test, pred) > bestAcuracyClassifier[1]:\n",
    "        bestAcuracyClassifier[0]= classifier\n",
    "        bestAcuracyClassifier[1] = accuracy_score(y_KNI_valid_test, pred)\n",
    "\n",
    "    print(classifier)\n",
    "    print()\n",
    "    print(\"accuracy:\", accuracy_score(y_KNI_valid_test, pred))\n",
    "    print(\"precision:\", precision_score(y_KNI_valid_test, pred, average='micro'))\n",
    "    print(\"recall:\", recall_score(y_KNI_valid_test, pred, average='macro'))\n",
    "    print()\n",
    "    print(classification_report(y_KNI_valid_test, pred))\n",
    "    print()\n",
    "    print(\"False Positive: {}\".format(falseP))\n",
    "    print(\"False Negative: {}\".format(falseN))\n",
    "    print('-----------------------------------------------------------------------------------------')\n",
    "print(bestAcuracyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestBase=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 10 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_SI_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 10, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "rf_KNI_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 10, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_SI_random.fit(X_SI_valid_train, y_SI_valid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "rf_KNI_random.fit(X_KNI_valid_train, y_KNI_valid_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_SI_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_KNI_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestRandomized=RandomForestClassifier(n_estimators= 400,min_samples_split= 5,min_samples_leaf= 1,max_features= \"sqrt\",max_depth= 30,bootstrap= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_SI_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2, return_train_score=True)\n",
    "grid_KNI_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SI_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNI_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SI_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNI_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestGrid=RandomForestClassifier(bootstrap=True, max_depth= 80,max_features= 2,min_samples_leaf= 3,min_samples_split= 8,n_estimators= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ownImplTreeBase = OwnTreeImplementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_split = [2, 5, 10, 15, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_leaf = [1, 2, 4, 6, 10, 12]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'min_split': min_samples_split,\n",
    "               'min_leaf': min_samples_leaf}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_max_depth = 0\n",
    "best_min_split = 0\n",
    "best_min_leaf = 0\n",
    "for depth in random_grid['max_depth']:\n",
    "    for split in random_grid['min_split']:\n",
    "        for leaf in random_grid['min_leaf']:\n",
    "            ot = OwnTreeImplementation(max_depth=depth, min_split=split, min_leaf=leaf)\n",
    "            ot.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "            pred = ot.predict(X_SI_valid_test)\n",
    "            acc = accuracy_score(y_SI_valid_test, pred)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_max_depth = depth\n",
    "                best_min_split = split\n",
    "                best_min_leaf = leaf\n",
    "\n",
    "print('acc: ', best_acc)\n",
    "print('max_depth: ', best_max_depth)\n",
    "print('min_split: ', best_min_split)\n",
    "print('min_leaf: ', best_min_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_max_depth = 0\n",
    "best_min_split = 0\n",
    "best_min_leaf = 0\n",
    "for depth in random_grid['max_depth']:\n",
    "    for split in random_grid['min_split']:\n",
    "        for leaf in random_grid['min_leaf']:\n",
    "            ot = OwnTreeImplementation(max_depth=depth, min_split=split, min_leaf=leaf)\n",
    "            ot.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "            pred = ot.predict(X_KNI_valid_test)\n",
    "            acc = accuracy_score(y_KNI_valid_test, pred)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_max_depth = depth\n",
    "                best_min_split = split\n",
    "                best_min_leaf = leaf\n",
    "\n",
    "print('acc: ', best_acc)\n",
    "print('max_depth: ', best_max_depth)\n",
    "print('min_split: ', best_min_split)\n",
    "print('min_leaf: ', best_min_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedOwntreeImplSI = OwnTreeImplementation(max_depth=10, min_leaf=4, min_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedOwntreeImplSI.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "pred=optimizedOwntreeImplSI.predict(X_SI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_SI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_SI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_SI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedOwntreeImpl.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "pred=optimizedOwntreeImpl.predict(X_KNI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_KNI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_KNI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_KNI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestGrid.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "pred=randomForestGrid.predict(X_SI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_SI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_SI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_SI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestRandomized.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "pred=randomForestRandomized.predict(X_SI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_SI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_SI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_SI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestBase.fit(X_SI_valid_train, y_SI_valid_train)\n",
    "pred=randomForestBase.predict(X_SI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_SI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_SI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_SI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestGrid.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "pred=randomForestGrid.predict(X_KNI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_KNI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_KNI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_KNI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestRandomized.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "pred=randomForestRandomized.predict(X_KNI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_KNI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_KNI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_KNI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestBase.fit(X_KNI_valid_train, y_KNI_valid_train)\n",
    "pred=randomForestBase.predict(X_KNI_valid_test)\n",
    "print(\"accuracy:\", accuracy_score(y_KNI_valid_test, pred))\n",
    "print(\"precision:\", precision_score(y_KNI_valid_test, pred, average='micro'))\n",
    "print(\"recall:\", recall_score(y_KNI_valid_test, pred, average='macro'))"
   ]
  },
  {
   "source": [
    "Vyssie mame porovnane klasifikatory z kniznice scikit s vlastnou implementaciou a vidime ze nasa vlastna implementacia ma horsiu accuracy ako prepripravene kniznice vidime vsak aj ze nasa vlastna implementacia s hlbkou 1 a teda vyuzitim iba jedneho atributu ma pomerne dobru presnost co naznacuje ze vysledok sa z velmi velkej casti odraza prave od jedneho atributu. zaujimave je ze v pripade Si datasetu bolo tymto atributom std_glucose a pre KNI dataset kurtosis_glucose toto moze byt dosledkom roznych metod nahradenia nanov. Na vyslednej accuracy sa to vsak neodrazilo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Vizualizacie jednotlivych klasifikatorov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizacia stromu pre randomForestgrid\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "graph = Source(export_graphviz(randomForestGrid,\n",
    "                               out_file=None,\n",
    "                               feature_names=X_KNI_valid.columns,\n",
    "                               class_names=[\"positive\", \"negative\"],\n",
    "                               filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "from IPython.display import HTML # toto je tu len pre to aby sa mi obrazok zmestil na obrazovku\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizacia stromu pre randomForestrandomized\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "graph = Source(export_graphviz(randomForestRandomized,\n",
    "                               out_file=None,\n",
    "                               feature_names=X_KNI_valid.columns,\n",
    "                               class_names=[\"positive\", \"negative\"],\n",
    "                               filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "from IPython.display import HTML # toto je tu len pre to aby sa mi obrazok zmestil na obrazovku\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizacia stromu pre randomForestBase\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n",
    "graph = Source(export_graphviz(randomForestBase,\n",
    "                               out_file=None,\n",
    "                               feature_names=X_KNI_valid.columns,\n",
    "                               class_names=[\"positive\", \"negative\"],\n",
    "                               filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "from IPython.display import HTML # toto je tu len pre to aby sa mi obrazok zmestil na obrazovku\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "source": [
    "# Porovnanie zvolenych strategii\n",
    "\n",
    "Vyuzili sme dve rozne metody doplnenie chybajucich hodnot a to pomocou simple imputeru a funkcie mean a doplnenie K najblizsich susedov\n",
    "\n",
    "V konecnom dosledku mozeme vidiet ze pre optimalizovane hyperparametre su vysledky pre obe mnoziny rovnake avsak pri SI mame v datasete iba 3 atributy a pri KNI 4 pri viacero atributoch v oboch datasetoch vznikala s narastajucou sa komplexitou narastajuca chyba co znacilo pretrenovanie mozeme teda povazovat dataset SI za menej komplexny a mneje robustny kedze sme ho museli viac zjednodusit aby sme dosiahli nezvysovanie sa chyby s komplexitou."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}